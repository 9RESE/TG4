{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TG4 - RL Agent Training\n",
    "## PPO for Crypto Accumulation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from data_fetcher import DataFetcher\n",
    "from models.rl_agent import TradingEnv, train_rl_agent, load_rl_agent\n",
    "from portfolio import Portfolio\n",
    "from backtester import Backtester\n",
    "from strategies.ripple_momentum_lstm import generate_ripple_signals\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fetch RLUSD-Inclusive Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetcher = DataFetcher()\n",
    "\n",
    "# Fetch main trading pairs\n",
    "symbols = ['XRP/USDT', 'BTC/USDT']\n",
    "data = {}\n",
    "\n",
    "for sym in symbols:\n",
    "    print(f\"Fetching {sym} from Kraken...\")\n",
    "    df = fetcher.fetch_ohlcv('kraken', sym, '1h', 2000)\n",
    "    if not df.empty:\n",
    "        data[sym] = df\n",
    "        print(f\"  Got {len(df)} candles\")\n",
    "\n",
    "# Also fetch RLUSD pairs\n",
    "print(\"\\nFetching RLUSD pairs...\")\n",
    "rlusd_data = fetcher.fetch_rlusd_pairs()\n",
    "data.update(rlusd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start TensorBoard in background (run in terminal: tensorboard --logdir=./tensorboard/)\n",
    "print(\"Training PPO agent on GPU (ROCm)...\")\n",
    "print(\"Monitor progress: tensorboard --logdir=./tensorboard/\")\n",
    "\n",
    "# Use subset for faster training demo\n",
    "training_data = {k: v for k, v in data.items() if k in ['XRP/USDT', 'BTC/USDT']}\n",
    "\n",
    "if training_data:\n",
    "    model = train_rl_agent(training_data, timesteps=50000)  # Quick training\n",
    "else:\n",
    "    print(\"No data available for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate RL Agent vs LSTM Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "try:\n",
    "    model = load_rl_agent()\n",
    "    print(\"RL model loaded successfully\")\n",
    "except:\n",
    "    print(\"No trained model found - run training cell first\")\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare RL vs LSTM backtest results\n",
    "if 'XRP/USDT' in data:\n",
    "    print(\"=== LSTM Strategy Backtest ===\")\n",
    "    signals = generate_ripple_signals(data, 'XRP/USDT')\n",
    "    bt = Backtester(data)\n",
    "    pf_lstm = bt.run_with_lstm_signals('XRP/USDT', signals)\n",
    "    print(f\"LSTM Total Return: {pf_lstm.total_return():.2%}\")\n",
    "    print(f\"LSTM Sharpe Ratio: {pf_lstm.sharpe_ratio():.2f}\")\n",
    "    print(f\"LSTM Max Drawdown: {pf_lstm.max_drawdown():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL Agent evaluation\n",
    "if model and training_data:\n",
    "    print(\"\\n=== RL Agent Evaluation ===\")\n",
    "    env = TradingEnv(training_data)\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "    \n",
    "    prices = env._current_prices()\n",
    "    final_value = env.portfolio.get_total_usd(prices)\n",
    "    \n",
    "    print(f\"RL Total Steps: {step}\")\n",
    "    print(f\"RL Total Reward: {total_reward:.4f}\")\n",
    "    print(f\"RL Final Portfolio Value: ${final_value:.2f}\")\n",
    "    print(f\"RL Return: {(final_value - 1000) / 1000:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot portfolio value over evaluation\n",
    "if model and training_data:\n",
    "    env = TradingEnv(training_data)\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    portfolio_values = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        prices = env._current_prices()\n",
    "        portfolio_values.append(env.portfolio.get_total_usd(prices))\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(portfolio_values)\n",
    "    plt.title('RL Agent Portfolio Value Over Time')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Portfolio Value (USD)')\n",
    "    plt.axhline(y=1000, color='r', linestyle='--', label='Starting Value')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

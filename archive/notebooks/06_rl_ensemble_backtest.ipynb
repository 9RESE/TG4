{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - RL Ensemble Backtest\n",
    "\n",
    "This notebook combines RL-based decisions with ensemble signals to backtest the full TG4 strategy.\n",
    "\n",
    "**Key Features:**\n",
    "- RL agent with target allocation rewards (BTC 40%, XRP 30%, RLUSD 20%)\n",
    "- Ensemble signal combination (LSTM + Arb + Rebalance)\n",
    "- RLUSD premium detection for ecosystem demand signals\n",
    "- Portfolio alignment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data_fetcher import DataFetcher\n",
    "from portfolio import Portfolio\n",
    "from models.rl_agent import TradingEnv, train_rl_agent, load_rl_agent\n",
    "from ensemble import Ensemble\n",
    "from strategies.ripple_momentum_lstm import generate_ripple_signals, check_rlusd_premium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fetch Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetcher = DataFetcher()\n",
    "\n",
    "symbols = ['XRP/USDT', 'BTC/USDT', 'RLUSD/USDT']\n",
    "data = {}\n",
    "\n",
    "for sym in symbols:\n",
    "    print(f\"Fetching {sym}...\")\n",
    "    df = fetcher.fetch_ohlcv('kraken', sym, '1h', 2000)\n",
    "    if not df.empty:\n",
    "        data[sym] = df\n",
    "        print(f\"  {len(df)} candles, latest: ${df['close'].iloc[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(data)} symbols\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Trading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting balance\n",
    "initial_balance = {'USDT': 1000.0, 'XRP': 500.0}\n",
    "\n",
    "# Create environment\n",
    "env = TradingEnv(data, initial_balance)\n",
    "\n",
    "print(\"Environment initialized\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Max steps: {env.max_steps}\")\n",
    "print(f\"\\nTarget allocation:\")\n",
    "for asset, weight in env.targets.items():\n",
    "    print(f\"  {asset}: {weight*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train RL Agent (if not already trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_path = '../src/models/rl_ppo_agent.zip'\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading existing model...\")\n",
    "    model = load_rl_agent('../src/models/rl_ppo_agent')\n",
    "    print(\"Model loaded!\")\n",
    "else:\n",
    "    print(\"No trained model found. Training new model...\")\n",
    "    print(\"This may take 5-10 minutes...\")\n",
    "    model = train_rl_agent(data, timesteps=100000)\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Backtest RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_rl_agent(model, data, initial_balance):\n",
    "    \"\"\"Run RL agent through historical data and track performance\"\"\"\n",
    "    env = TradingEnv(data, initial_balance)\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    portfolio_values = []\n",
    "    allocations = []\n",
    "    actions_taken = []\n",
    "    alignment_scores = []\n",
    "    \n",
    "    done = False\n",
    "    step = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        action = int(action)\n",
    "        actions_taken.append(action)\n",
    "        \n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        # Track portfolio value\n",
    "        prices = env._current_prices()\n",
    "        total_value = env.portfolio.get_total_usd(prices)\n",
    "        portfolio_values.append(total_value)\n",
    "        \n",
    "        # Track allocation\n",
    "        alloc = {}\n",
    "        for asset in ['BTC', 'XRP', 'RLUSD', 'USDT']:\n",
    "            val = env.portfolio.balances.get(asset, 0) * prices.get(asset, 1.0)\n",
    "            alloc[asset] = val / total_value if total_value > 0 else 0\n",
    "        allocations.append(alloc)\n",
    "        \n",
    "        # Calculate alignment score\n",
    "        score = sum(min(alloc.get(a, 0), env.targets.get(a, 0)) for a in env.targets)\n",
    "        alignment_scores.append(score)\n",
    "        \n",
    "        step += 1\n",
    "        if step % 200 == 0:\n",
    "            print(f\"Step {step}: Value=${total_value:.2f}, Alignment={score:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'values': portfolio_values,\n",
    "        'allocations': allocations,\n",
    "        'actions': actions_taken,\n",
    "        'alignment': alignment_scores,\n",
    "        'final_portfolio': env.portfolio\n",
    "    }\n",
    "\n",
    "print(\"Running RL backtest...\")\n",
    "results = backtest_rl_agent(model, data, initial_balance)\n",
    "print(f\"\\nBacktest complete: {len(results['values'])} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.array(results['values'])\n",
    "initial_value = 1000 + 500 * data['XRP/USDT']['close'].iloc[60]  # Approx initial\n",
    "\n",
    "# Calculate metrics\n",
    "total_return = (values[-1] / initial_value - 1) * 100\n",
    "max_value = values.max()\n",
    "drawdowns = (max_value - values) / max_value\n",
    "max_drawdown = drawdowns.max() * 100\n",
    "\n",
    "# Daily returns for Sharpe\n",
    "returns = np.diff(values) / values[:-1]\n",
    "sharpe = np.sqrt(252 * 24) * returns.mean() / (returns.std() + 1e-8)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"RL AGENT BACKTEST RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Initial Value:    ${initial_value:.2f}\")\n",
    "print(f\"Final Value:      ${values[-1]:.2f}\")\n",
    "print(f\"Total Return:     {total_return:.2f}%\")\n",
    "print(f\"Max Drawdown:     {max_drawdown:.2f}%\")\n",
    "print(f\"Sharpe Ratio:     {sharpe:.2f}\")\n",
    "print(f\"Avg Alignment:    {np.mean(results['alignment']):.3f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio value over time\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# Value chart\n",
    "axes[0].plot(values, 'b-', linewidth=1)\n",
    "axes[0].axhline(y=initial_value, color='gray', linestyle='--', label='Initial')\n",
    "axes[0].set_title('Portfolio Value Over Time')\n",
    "axes[0].set_ylabel('USD Value')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Allocation over time\n",
    "alloc_df = pd.DataFrame(results['allocations'])\n",
    "axes[1].stackplot(range(len(alloc_df)), \n",
    "                   alloc_df['BTC'], alloc_df['XRP'], \n",
    "                   alloc_df['RLUSD'], alloc_df['USDT'],\n",
    "                   labels=['BTC', 'XRP', 'RLUSD', 'USDT'],\n",
    "                   alpha=0.8)\n",
    "axes[1].axhline(y=0.4, color='black', linestyle='--', alpha=0.5)\n",
    "axes[1].axhline(y=0.7, color='black', linestyle='--', alpha=0.5)\n",
    "axes[1].axhline(y=0.9, color='black', linestyle='--', alpha=0.5)\n",
    "axes[1].set_title('Asset Allocation Over Time')\n",
    "axes[1].set_ylabel('Weight')\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "# Alignment score\n",
    "axes[2].plot(results['alignment'], 'g-', linewidth=1)\n",
    "axes[2].axhline(y=1.0, color='green', linestyle='--', label='Perfect (1.0)')\n",
    "axes[2].set_title('Alignment Score (vs Target Allocation)')\n",
    "axes[2].set_ylabel('Score')\n",
    "axes[2].set_xlabel('Step')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/rl_backtest_results.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Action Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_names = {\n",
    "    0: 'BTC Buy', 1: 'BTC Hold', 2: 'BTC Sell',\n",
    "    3: 'XRP Buy', 4: 'XRP Hold', 5: 'XRP Sell',\n",
    "    6: 'RLUSD Buy', 7: 'RLUSD Hold', 8: 'RLUSD Sell'\n",
    "}\n",
    "\n",
    "actions = results['actions']\n",
    "action_counts = pd.Series(actions).value_counts().sort_index()\n",
    "\n",
    "print(\"\\nAction Distribution:\")\n",
    "print(\"-\"*40)\n",
    "for action_id, count in action_counts.items():\n",
    "    pct = count / len(actions) * 100\n",
    "    print(f\"{action_names[action_id]:15} : {count:5d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Bar chart\n",
    "plt.figure(figsize=(12, 5))\n",
    "colors = ['#ff6b6b', '#ffd93d', '#6bcb77'] * 3\n",
    "bars = plt.bar([action_names[i] for i in range(9)], \n",
    "               [action_counts.get(i, 0) for i in range(9)],\n",
    "               color=colors)\n",
    "plt.title('RL Agent Action Distribution')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare with Ensemble Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ensemble strategy for comparison\n",
    "portfolio = Portfolio(initial_balance)\n",
    "ensemble = Ensemble(data, portfolio)\n",
    "\n",
    "ensemble_values = []\n",
    "ensemble_signals = []\n",
    "\n",
    "# Simulate through the same period\n",
    "for i in range(60, min(len(data['XRP/USDT']), 60 + len(results['values']))):\n",
    "    # Get prices at this step\n",
    "    prices = {'USDT': 1.0, 'USDC': 1.0, 'RLUSD': 1.0}\n",
    "    for sym, df in data.items():\n",
    "        if i < len(df):\n",
    "            base = sym.split('/')[0]\n",
    "            prices[base] = df['close'].iloc[i]\n",
    "    \n",
    "    total = portfolio.get_total_usd(prices)\n",
    "    ensemble_values.append(total)\n",
    "    \n",
    "    # Get signal (without executing - just tracking)\n",
    "    signal = ensemble.get_signal('XRP/USDT')\n",
    "    ensemble_signals.append(signal['action'])\n",
    "\n",
    "print(f\"Ensemble simulation: {len(ensemble_values)} steps\")\n",
    "print(f\"Buy signals: {ensemble_signals.count('long_xrp')}\")\n",
    "print(f\"Sell signals: {ensemble_signals.count('reduce_xrp')}\")\n",
    "print(f\"Hold signals: {ensemble_signals.count('hold')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare RL vs Buy-and-Hold vs Ensemble\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Normalize to 100 for comparison\n",
    "rl_norm = 100 * np.array(results['values']) / results['values'][0]\n",
    "bh_norm = 100 * np.array(ensemble_values) / ensemble_values[0] if ensemble_values else [100]\n",
    "\n",
    "ax.plot(rl_norm, 'b-', linewidth=2, label='RL Agent')\n",
    "ax.plot(bh_norm[:len(rl_norm)], 'gray', linewidth=1, linestyle='--', label='Buy & Hold')\n",
    "ax.axhline(y=100, color='black', linestyle=':', alpha=0.5)\n",
    "\n",
    "ax.set_title('RL Agent vs Buy & Hold Performance')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Normalized Value (100 = start)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Holdings Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_portfolio = results['final_portfolio']\n",
    "final_prices = {\n",
    "    'USDT': 1.0, 'USDC': 1.0, 'RLUSD': 1.0,\n",
    "    'BTC': data['BTC/USDT']['close'].iloc[-1],\n",
    "    'XRP': data['XRP/USDT']['close'].iloc[-1]\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL HOLDINGS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "total = 0\n",
    "for asset, amount in final_portfolio.balances.items():\n",
    "    if amount > 0.0001:\n",
    "        price = final_prices.get(asset, 1.0)\n",
    "        value = amount * price\n",
    "        total += value\n",
    "        print(f\"{asset:8}: {amount:12.4f} (${value:,.2f})\")\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(f\"{'TOTAL':8}: ${total:,.2f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Allocation pie chart\n",
    "holdings = {}\n",
    "for asset, amount in final_portfolio.balances.items():\n",
    "    if amount > 0.0001:\n",
    "        holdings[asset] = amount * final_prices.get(asset, 1.0)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(holdings.values(), labels=holdings.keys(), autopct='%1.1f%%',\n",
    "        colors=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff66b3'])\n",
    "plt.title('Final Portfolio Allocation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. RLUSD Premium Detection Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'RLUSD/USDT' in data:\n",
    "    rlusd_prices = data['RLUSD/USDT']['close']\n",
    "    \n",
    "    premium_periods = (rlusd_prices > 1.001).sum()\n",
    "    discount_periods = (rlusd_prices < 0.999).sum()\n",
    "    \n",
    "    print(\"RLUSD Price Analysis\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"Current Price:   ${rlusd_prices.iloc[-1]:.4f}\")\n",
    "    print(f\"Mean Price:      ${rlusd_prices.mean():.4f}\")\n",
    "    print(f\"Std Dev:         ${rlusd_prices.std():.4f}\")\n",
    "    print(f\"Premium periods: {premium_periods} ({premium_periods/len(rlusd_prices)*100:.1f}%)\")\n",
    "    print(f\"Discount periods:{discount_periods} ({discount_periods/len(rlusd_prices)*100:.1f}%)\")\n",
    "    \n",
    "    # Test premium detection\n",
    "    is_premium = check_rlusd_premium(data, threshold=1.001)\n",
    "    print(f\"\\nCurrent premium status: {'PREMIUM' if is_premium else 'Normal'}\")\n",
    "else:\n",
    "    print(\"RLUSD/USDT data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key findings from this backtest:\n",
    "1. RL agent learns to balance accumulation with profit-taking\n",
    "2. Alignment score shows how well we're tracking target allocation\n",
    "3. RLUSD premium detection can signal ecosystem demand\n",
    "4. Combination of RL + Ensemble provides robust signals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
